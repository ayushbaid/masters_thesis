\chapter{Formulation}

We will now model the system as well as its variables using MRFs. We will introduce priors on the variables. We will then derive the optimization objective.

$X$ is an uncorrupted image instance, which we want to estimate. $Y$ is the observed image, $T$ is the smoke transmission map at hand, and $R$ is the speckle map. $K_{spec}$ and $K_{smoke}$ are speckle and smoke color respectively.

\section{Image formation}
The artifacts are captured in a 3 step fashion. In this section, $i$ denotes the pixel location. \Eqref{eqn:speckle} captures the effect of speckles using a binary speckle map $R$, where $R_i$ having a value $1$ denotes the presence of speckle. \Eqref{eqn:smokenoise} captures the smoke using smoke map $T, T_i \in [0, 1]$ and an i.i.d white Gaussian noise $n_i$ is added at each pixel.

\begin{align}
    Z_i &= (1 - R_i) X_i  + R_i K_{spec} \label{eqn:speckle} \\
    Y_i &= T_i Z_i + (1 - T_i) K_{smoke} + \eta_i \label{eqn:smokenoise}
\end{align}

\section{Variable modeling}
We have two variables to model: the original uncorrupted image and the smoke transmission map. They are denoted by Markov random fields (MRFs) $\mathbf{X}$ and $\mathbf{R}$ respectively. If there are $I$ pixels in the underlying image, then
\begin{align}
    \mathbf{X} := \lbrace X_i \rbrace_{i=1}^{I} \\
    \mathbf{T} := \lbrace T_i \rbrace_{i=1}^{I} 
\end{align}
where $X_i \in [0, 1]^3$ is a vector valued random variable denoting a value in RGB color space, and $T_i \in [0, 1]$ is a scalar random variable denoting the smoke transmission coefficient.

We will now introduce the priors on $\mathbf{X}$ and $\mathbf{T}$. We want $\mathbf{X}$ to have texture to tackle the speckles, and to have high contrast to counter the smoke. We want $\mathbf{T}$ to be spatially smooth.

\subsection{Sparse coding under a dictionary}

Given a data matrix $\mathbf{X}$, whose columns are i.i.d. random vectors, a linear decomposition is of the form 
\begin{align}
    \mathbf{X} \approx \mathbf{D} \mathbf{S} \label{eqn:lineardecomp}
\end{align}
In \Eqref{eqn:lineardecomp}, $\mathbf{D}$ is the dictionary and $\mathbf{S}$ is a code of $\mathbf{X}$.

In sparse coding \cite{harpur1996sc}, we want an input vector to be using as few atoms of the dictionary as possible. A common way to enforce sparsity is regularization. $\lambda$ in \Eqref{eqn:sparsecoding} controls the tradeoff between sparseness and reconstruction accuracy. \Eqref{eqn:sparsecoding} is minimized to obtain the best linear sparse representation.

\begin{align}
    J(\mathbf{D}, \mathbf{S}) = \dfrac{1}{2} || \mathbf{X} - \mathbf{D} \mathbf{S} ||_2^2 + \lambda \sum_{ij} f(S_{ij}) \label{eqn:sparsecoding}
\end{align}
The sparsity function $f$ typically is strictly increasing function of the absolute value of its argument. During optimization, there is a decrease in optimization objective by simply scaling up the dictionary atoms and decreasing the value of coefficient in the code commensurately. To prevent this blowup, there is a constraint on the norm of dictionary atoms.


Non-negative sparse coding \cite{hoyer2002nnsc} can be applied when $\mathbf{X}$ is non-negative. It constrains $\mathbf{D}$ and $\mathbf{C}$ to be non-negative. The choice of sparsity function is $f(S_{ij}) = | S_{ij} | = S_{ij}$. The main intuition behind introducing non-negative constraint is that in many data classes like images, parts combine additively to form a whole as opposed to canceling each other.

\cite{mairal2009online} proposed an online algorithm for dictionary learning in the non-negative sparse coding framework. They use least angle regression to solve the sparse coding problem, given a dictionary. Their algorithm and accompanying code provide a significant speedup for learning a  dictionary on large dataset.


\subsection{Image intensity distribution}
Smoke, generally gray, damages the color in the image. We need some model distribution of colors in uncorrupted high quality images to compare our image at hand with. A distribution in the combined \textit{RGB} space is the best model but it gives rise to high complexity for its representation as no generic distribution can fit our dataset.

The channels in the \textit{RGB} space exhibit very high correlation and modeling the channels independently is a poor choice. We first transform the data from \textit{RGB} space to \textit{LMS} space, as the latter is more closely related to human perception. We then calculate the eigenvectors and use them as the basis vectors for the new space, which we call \textit{l$\alpha\beta$} space. The final transformation is 
\begin{align}
    \begin{bmatrix}
    l \\ \alpha \\ \beta
    \end{bmatrix}
    = 
    \begin{bmatrix}
    0.3568 & 0.8413 & 0.5304 \\
    0.0760 & -0.2006 & 0.1239 \\
    0.2267 & 0.3574 & -0.6512  
    \end{bmatrix}
    \begin{bmatrix}
    R \\ G \\ B
    \end{bmatrix}
\end{align}
% TODO: correct

\begin{figure}[!t]
    \threeAcrossHeight{graphics/dist/axis_1.png}{graphics/dist/axis_2.png}{graphics/dist/axis_3.png}{0.33}
    \caption
    {
        %
        {\bf Learning Prior PDFs on Color.}
        %
        Empirical histograms (bar plots) and fitted parametric PDFs (solid curves) in uncorrupted laparoscopic images, for 3 channel components: {\bf
            (a)}~gamma $\Gamma_1$, {\bf (b)}~Gaussian $G_2$, {\bf (c)}~Gaussian $G_3$.
        %
    }
    \label{fig:intensity_dist}
\end{figure}


\subsection{KS statistic}
Kolmogorov-Smirnov (KS) statistic is used to compare a sample with a reference probability distribution. Given an empirical cumulative distribution function (CDF) $F_{emp}$ and reference CDF $F_{ref}$, the KS distance is
\begin{align}
    KS(F^{emp}; F^{ref}) = \max_{k} | F^{emp}(k) - F^{ref}(k) | \label{eqn:ks_distance}
\end{align}
KS distance has low computational complexity but it is not differentiable. We can approximate the gradient as $k - \text{cdfmatch}(F^{emp}; F^{ref})(k)$, where cdfmatch is the CDF matching function.

On our data, we will generate the empirical distribution $F_\mathbf{x}$, evaluate the KS statistic independently for the three channels and add them up. The total penalty will be
\begin{align}
    J_{KS}(\mathbf{x}) = \sum_{i=1}^{3} KS(F^{\mathbf{x}}_{c_i}; F^{ref}_{c_i}) \label{eqn:cost_ks}
\end{align}
where $c_i$ is channel number $i$.

\subsection{Kernel density estimation}
Kernel density estimation (KDE) is used to estimate the probability density function in a non-parametric way. Let $ \mathbf{x} = \left( x_1, x_2, ..., x_n\right)$ be i.i.d. samples from a distribution $f$. The kernel density estimate is
\begin{align}
    f^\mathbf{x} (b) = \frac{1}{nh} \sum_{i=1}^{n} K\left(\frac{b-x_i}{h}\right) \label{eqn:kde}
\end{align}
where $K(.)$ is a kernel which should take non-negative values, integrate to one, and have mean zero. $h > 0$ is the bandwidth and controls the trade-off between bias and variance of the . We will use the Gaussian kernel due to its mathematical property like differentiability. Due to our choice of Gaussian kernel, we can use the rule of thumb estimate \cite{silverman1986density} for bandwidth using the standard deviation of samples $\hat{\sigma}$. \Eqref{eqn:kdebw} is used to tune the bandwidth using training data.
\begin{align}
    h = \left( \frac{4 \hat{\sigma}^5}{3n}\right)^{\frac{1}{5}} \label{eqn:kdebw}
\end{align}

We will now derive the CDF estimator and its gradients using the Gaussian kernel.
\begin{align}
    f^\mathbf{x} (b) &= \frac{1}{nh \sqrt{2 \pi \sigma^2}} \sum_{i=1}^{n} \exp \left( -\frac{(b-x_i)^2} {2 \sigma^2 h^2}\right) \\
    F^\mathbf{x} (b) &= \sum_{s=x_{min}}^{b} f^\mathbf{x} (s) \label{eqn:kdecdf} \\
    % grads
    \frac{\partial f^\mathbf{x} (b)}{\partial x_i} &=  \frac{1}{nh \sqrt{2 \pi \sigma^2}} \exp \left( -\frac{(b-x_i)^2} {2 \sigma^2 h^2}\right) \frac{ \left( b - x_i \right) }{ \sigma^2 h^2} \\
    \frac{\partial F^\mathbf{x} (b)} {\partial x_i} &= \sum_{s=x_{min}}^{b} \frac{\partial f^\mathbf{x}_h (s)}{\partial x_i}
\end{align}

We will now introduce the cost function $J_{dist}$ in \Eqref{eqn:kdecost} and its gradient, where $H(\cdot)$ in evaluates the transformation of a point under CDF matching of the empirical distribution with the reference (\Eqref{eqn:cdfinv}). The choice of the optimization objective was made as it exhibits better convergence when gradient descent is used.
\begin{align}
    J_{dist} (\mathbf{x}) &= \sum_{b \in B} \left\lbrace b - H \left( b; F^\mathbf{x}, F^{ref} \right) \right\rbrace^2 \label{eqn:kdecost} \\
    \frac{ \partial J_{dist} (\mathbf{x})} { \partial x_i} &= 2 \sum_{b \in B} \left\lbrace b - H \left( b; F^\mathbf{x}, F_{ref} \right) \right\rbrace \frac{\partial H \left( b; F^\mathbf{x}, F_{ref} \right)}{\partial x_i}
\end{align}
\begin{align}
    H \left( b; F^\mathbf{x}, F_{ref} \right) &= F_{ref}^{-1} \left( F^\mathbf{x} (b) \right) \label{eqn:cdfinv} \\
    \frac{\partial H \left( b; F^\mathbf{x}, F_{ref} \right)}{\partial x_i} &= F_{ref}^{-1} \left( F^\mathbf{x} (b) \right) \frac{ \partial F^\mathbf{x} (b; x_1^n)} { \partial x_i}
\end{align}

On our data, we will add up the penalty for each channel $c_i$ independently.
\begin{align}
    J_{KDE}(\mathbf{x}) = \sum_{i=1}^{3} \sum_{b \in B_{c_i}} \left\lbrace b - H \left( b; F^\mathbf{x}_{c_i}, F^{ref}_{c_i} \right) \right\rbrace^2 \label{eqn:cost_kde}
\end{align}

\subsection{Prior on image}
% TODO info about MRF



\subsection{Spatial smoothness prior}
To enforce spatial smoothness prior, we will penalize deviations in a local neighborhood. The MRF $\mathbf{T}$ is defined with a neighborhood system $\mathcal{N}^T := \lbrace \mathcal{N}_i^T \rbrace$, where $\mathcal{N}_i^T$ is the set of 4 nearest neighbors at pixel $i$. The prior distribution is defined as
\begin{align}
    P(\mathbf{T}) = \frac{1}{Z} \exp \left( - \sum_{i=1}^{I} \sum_{j \in \mathcal{N}_i^T} \gamma_3 (T_i - T_j)^2 \right) \label{eqn:spatialsmoothness}
\end{align}
where the outer sum is performed over all pixels, and $\gamma \in \mathbb{R}^+$ is a free parameter.

% TODO: weights?