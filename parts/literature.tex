
\chapter{Literature Survey}

To the best of our knowledge, no existing work tackles smoke, specular highlights, and noise in a joint setting. We will cover these three and some related problem separately. First, we will look into specular highlights removal in laparoscopy images, which is mostly tackled as inpainting problem using some form of averaging. Inpainting is a process of filling in missing information, usually using true information in the surroundings. Then, we will cover dehazing, which is haze removal in outdoor images and bears similarity with desmoking in laparoscopy images. This will be followed with desmoking itself. We will not cover denoising as an independent domain.

\section{Speckle removal in laparoscopy images}
Arnold et al.\cite{arnold2010speckle} use a 2-step inpainting process. In the first step, they fill in the missing data with the centroid of available data in the local surroundings and perform strong smoothing using a Gaussian kernel. The smooth image output of the first step and the original image is combined using a weight mask in the second step. The weight mask has high weights near the speckles and decays non-linearly with distance. This results in a gradual transition between original image and the smooth median filtered image. The results however, are smooth and lack texture. This is expected because median filtering is not suitable to interpolate texture.

Isotropic color diffusion is used by Saint-Pierre et al. \cite{saint2011detection}. They use discrete convolutions with a kernel repeatedly until convergence is reached. The results, however, do not maintain edges and sharp texture. This is expected from an isotropic diffusion process. Stoyanov and Yang \cite{stoyanov2005removing} use temporal non-rigid registration to obtain pixel values lost due to speckles. The location of speckles shift with time, and hence missing data in one frame may be present in other frames. Interpolation is performed using control points obtained after registration of frames captured at different instances. This method is non-temporal in nature, and also the averaging leads to over-smooth inpainting.

\section{Dehazing in outdoor images}
Outdoor images, particularly of landscapes are often plagued by haze. Haze can be natural (fog) or artificial due to pollution. Haze corrupts the color of image, and when present in large concentration, it can completely obscure the subjects. The effect of haze is modeled by a linear combination of object's radiance and haze color \cite{koschmieder1925smokemodel}. The following equation, ubiquitous in literature, captures the effect of haze
\begin{align}
    Y_i = T_i X_i + \left( 1 - T_i \right) A \label{eqn:hazemodel}
\end{align}
where at pixel location $i$, $Y_i$ is observed image pixel, $T_i \in [0, 1]$ is the haze transmission coefficient, $X_i$ is radiance of the scene sans haze, and $A$ is the airlight (considered constant for all pixels). 

An important property which is exploited quite often is that the haze transmission coefficient $\lbrace T_i \rbrace$ is directly proportional to scene depth, and is hence spatially smooth. Fattal \cite{fattal2008single} uses Markov random field (MRF) to model the transmission map. Squared difference for four nearest-neighbors for each pixel location is penalized to enforce spatial regularity. Along with MRF for spatial smoothness, Tan \cite{tan2008visibility} also increases image contrast by optimizing for the number of edges in the image. Both the methods do not utilize any information about the distribution of colors in the image.

He et al. \cite{he2011dark} observe a statistical property that most local patches in outdoor haze-free images contain some pixel that has low intensity in at least one color channel. Infact, the lowest intensity in a local patch is called the \emph{dark channel} and serves as an estimate for the transmission coefficient at that location. Soft matting is used to obtain a smooth final estimate of transmission map in \cite{he2011dark}. Pang et al. \cite{pang2011improved} use adaptive patch size and replace the soft matting step with guided filtering. Gibson and Nguyen \cite{gibson2013wiener} calculate the dark channel and then apply adaptive wiener filter to smooth out the transmission map. Matlin and Milanfar \cite{matlin2012removal} argue that the \emph{dark channel} will be susceptible to outliers resulting from noise. They propose an iterative non-parametric kernel regression. The optimization is performed by alternating between minimization in terms of the transmission map and the uncorrupted image estimate to obtain simultaneous dehazing and denoising. While the dark channel principle is also applicable to laparoscopy images, we can derive more stringent properties to work with. For example, in laparoscopy images, the red channel usually contains the maximum value at each pixel out of the three color channels. 

124 images are used by Joshi and Cohen \cite{joshi2010multi} to generate a final image performing weighted averaging of transitionally aligned images. Mt. Rainer, the subject of interest in the paper, has large white glaciers which do not obey the local dark channel property. They also assume airlight constant for a scan-line and not for the whole image and compute the dark channel value per scan-line. This is then used to compute the transmission map and dehaze the image. This method is impractical for laparoscopy as it requires large number of images for a subject. 




\section{Desmoking in laparoscopy images}
Kotwal et al. \cite{kotwal2016joint} use MRFs to model the image and smoke transmission map and perform joint optimization for smoke and noise removal. They penalize distance between the empirical distribution and template distribution of image intensities and combine it with Huber penalty in local patches to create a prior on the uncorrupted image. Spatial smoothness prior is used on transmission map.












%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../mainrep"
%%% End: 
