
\chapter{Literature Survey}

To the best of our knowledge, no existing work tackles smoke, specular highlights, and noise in a joint setting. We will cover these three and some related problem separately. First, we will look into specular highlights removal in laparoscopy images, which is mostly tackled as an inpainting problem. Inpainting is a process in filling in missing information, usually using true information in the surroundings. Then, we will cover dehazing, which is haze removal in outdoor images and bears similarity with desmoking in laparoscopic images. This will be followed with desmoking itself. We will not cover denoising as an independent domain.

\section{Speckle removal in laparoscopy images}
\cite{arnold2010speckle} use a 2-step inpainting process. In the first step, they fill in the missing data by the centroid of available data within a certain distance and perform strong smoothing using a Gaussian kernel. The smooth image output of the first step and the original image is combined using a weight mask in step 2. The weight mask has high weights near the speckles and decays non-linearly with distance. This results in a gradual transition between original image and the smooth median filtered image. The results however, are smooth and lack texture. This is expected because median filtering is not suitable to interpolate texture.

Isotropic color diffusion is used by \cite{saint2011detection}. They use discrete convolutions with a kernel repeatedly until convergence is reached. \cite{stoyanov2005removing} use temporal non-rigid registration to obtain pixel values lost due to speckles. The location of speckles shift with time, and hence missing data can be interpolated by control points obtained after registration with frames captured at different instances. Both the methods perform averaging for inpainting and hence are unable to fill in texture.

\section{Dehazing in outdoor images}
Outdoor images, particularly of landscapes are often plagued by haze. Haze can be natural (fog) or artificial due to pollution. Haze corrupts the color of image, and when present in large concentration, it can completely obscure the subjects.

The effect of haze is modeled by a linear combination of object's radiance and haze color \cite{koschmieder1925smokemodel}. The following equation,ubiqutous in literature, captures the effect of haze.
\begin{align}
    Y_i = T_i X_i + \left( 1 - T_i \right) A \label{eqn:hazemodel}
\end{align}
where at pixel location $i$, $Y_i$ is observed image pixel, $T_i \in [0, 1]$ is the haze transmission coefficient, $X_i$ is radiance of the scene sans haze, and $A$ is the airlight (considered constant for all pixels). An important property which is exploited quite often is that the haze transmission coefficient $\lbrace T_i \rbrace$ is directly proportional to scene depth, and is hence spatially smooth.

\cite{fattal2008single} use Markov random field (MRF) to model the transmission map. Squared difference for four nearest-neighbors for each pixel location is penalized to enforce spatial regularity. Spatial regularity of transmission map is also used by \cite{tan2008visibility} as a prior for the MRF model. The image contrast is associated with the number of edges and is optimized for to get haze free high contrast images. Both the methods do not utilize any information about the distribution of colors in the image.

\cite{he2011dark} observe a statistical property that most local patches in outdoor haze-free images contain some pixels that have low intensities in at least one color channel. Infact, the lowest intensity in a local patch is called \emph{dark channel} and serves as an estimate for the transmission coefficient at that location. Soft matting is used to obtain a smooth final estimate of transmission map. Airlight is estimated by the top 0.1 percent brightest pixel in the dark channel. \cite{pang2011improved} use adaptive patch size and replace the soft matting step with guided filtering. \cite{gibson2013wiener} calculate the dark channel and then apply adaptive wiener filter to smooth out the transmission map. \cite{matlin2012removal} argue that the \textit{dark channel} will be susceptible to outliers resulting from noise. They propose an iterative non-parametric kernel regression. The optimization is performed by alternating between minimization in terms of the transmission map and the uncorrupted image estimate to obtain simultaneous dehazing and denoising. While the dark channel principle is also applicable to laparoscopy images, we can derive more stringent properties to work with. For example, in laparoscopy images, the red channel usually contains the maximum value at each pixel out of the three color channels. 

124 images are used by \cite{joshi2010multi} to generate a final image performing weighted averaging of transitionally aligned images. Mt. Rainer, the subject of interest in the paper has large white glaciers which do not obey the local dark channel property. They also assume airlight constant for a scan-line and not for the whole image and compute the dark channel value per scan-line. This is then used to compute the transmission map and dehaze the image. This method is impractical for laparoscopy as it requires large number of images for a subject. 




\section{Desmoking in laparoscopy images}
\cite{kotwal2016joint} used MRFs to model the image and smoke transmission map and performed joint optimization for smoke and noise removal. They penalized distance between the empirical distribution and template disribution of image intensities and combine it with huber penalty in local patches to create a prior on the uncorrupted image. Spatial smoothness prior is used on transmission map












%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../mainrep"
%%% End: 
